{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63ed0b21",
   "metadata": {},
   "source": [
    "# IA Training — Feature Encoding & Similarity Benchmark\n",
    "\n",
    "Ce notebook encode toutes les images de `casting_data/` avec le même pipeline ResNet50 + StandardScaler utilisé par le backend, puis benchmark plusieurs métriques de distance pour trouver la meilleure recherche de similarité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6722ae21",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.12' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "assert sys.version_info[:2] == (3, 12), f\"Expected Python 3.12, got {sys.version_info[:2]}\"\n",
    "\n",
    "# Install exact same versions as backend Dockerfile\n",
    "!pip install --no-cache-dir torch==2.9.0+cpu torchvision==0.24.0+cpu --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install --no-cache-dir scikit-learn==1.6.1 scipy joblib numpy\"<2\" Pillow matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf0bef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from joblib import load\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Add backend to path so we can import FeatureExtractor\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'backend'))\n",
    "from feature_extractor import FeatureExtractor\n",
    "\n",
    "# --- Configuration ---\n",
    "CASTING_DATA_DIR = Path('casting_data')\n",
    "MODELS_DIR = Path('models')\n",
    "RESNET_WEIGHTS = MODELS_DIR / 'resnet50_extractor.pth'\n",
    "SCALER_PATH = MODELS_DIR / 'scaler.joblib'\n",
    "OUTPUT_NPZ = MODELS_DIR / 'features_dataset.npz'\n",
    "OUTPUT_CONFIG = MODELS_DIR / 'similarity_config.json'\n",
    "IMG_SIZE = 224\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'Device: {DEVICE}')\n",
    "print(f'Casting data dir: {CASTING_DATA_DIR} (exists={CASTING_DATA_DIR.exists()})')\n",
    "print(f'ResNet weights: {RESNET_WEIGHTS} (exists={RESNET_WEIGHTS.exists()})')\n",
    "print(f'Scaler: {SCALER_PATH} (exists={SCALER_PATH.exists()})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22725300",
   "metadata": {},
   "source": [
    "## 1. Chargement du modèle et du scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe5d0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature extractor (same as backend)\n",
    "extractor = FeatureExtractor(model_name='resnet50', weights_path=str(RESNET_WEIGHTS))\n",
    "extractor = extractor.to(DEVICE)\n",
    "extractor.eval()\n",
    "print(f'FeatureExtractor loaded on {DEVICE}')\n",
    "\n",
    "# Load scaler (same as backend)\n",
    "scaler = load(SCALER_PATH)\n",
    "print(f'Scaler loaded: {type(scaler).__name__}')\n",
    "\n",
    "# Same transform as backend (test_transform)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7635cf1a",
   "metadata": {},
   "source": [
    "## 2. Collecte et encodage de toutes les images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce95533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_images(base_dir: Path):\n",
    "    \"\"\"Recursively collect all image paths and infer labels from directory names.\"\"\"\n",
    "    images = []\n",
    "    for img_path in sorted(base_dir.rglob('*.jpeg')):\n",
    "        # Infer label from parent directory name\n",
    "        parent = img_path.parent.name\n",
    "        if 'ok' in parent:\n",
    "            label = 'ok'\n",
    "        elif 'def' in parent:\n",
    "            label = 'def'\n",
    "        else:\n",
    "            label = 'unknown'\n",
    "        # Store relative path from casting_data/\n",
    "        rel_path = str(img_path.relative_to(base_dir))\n",
    "        images.append((img_path, rel_path, label))\n",
    "    \n",
    "    # Also check for .jpg and .png\n",
    "    for ext in ['*.jpg', '*.png']:\n",
    "        for img_path in sorted(base_dir.rglob(ext)):\n",
    "            parent = img_path.parent.name\n",
    "            if 'ok' in parent:\n",
    "                label = 'ok'\n",
    "            elif 'def' in parent:\n",
    "                label = 'def'\n",
    "            else:\n",
    "                label = 'unknown'\n",
    "            rel_path = str(img_path.relative_to(base_dir))\n",
    "            images.append((img_path, rel_path, label))\n",
    "    \n",
    "    return images\n",
    "\n",
    "all_images = collect_images(CASTING_DATA_DIR)\n",
    "print(f'Total images found: {len(all_images)}')\n",
    "\n",
    "# Count by label\n",
    "from collections import Counter\n",
    "label_counts = Counter(label for _, _, label in all_images)\n",
    "print(f'Labels: {dict(label_counts)}')\n",
    "\n",
    "# Show first few\n",
    "for _, rel, label in all_images[:5]:\n",
    "    print(f'  {label:4s} | {rel}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949ade60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_images(image_list, extractor, scaler, transform, device, batch_size=32):\n",
    "    \"\"\"Encode all images through ResNet50 + Scaler. Returns features matrix.\"\"\"\n",
    "    all_features = []\n",
    "    all_paths = []\n",
    "    all_labels = []\n",
    "    errors = []\n",
    "    \n",
    "    total = len(image_list)\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # Process in batches for efficiency\n",
    "    for i in range(0, total, batch_size):\n",
    "        batch = image_list[i:i+batch_size]\n",
    "        batch_tensors = []\n",
    "        batch_paths = []\n",
    "        batch_labels = []\n",
    "        \n",
    "        for img_path, rel_path, label in batch:\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                tensor = transform(img)\n",
    "                batch_tensors.append(tensor)\n",
    "                batch_paths.append(rel_path)\n",
    "                batch_labels.append(label)\n",
    "            except Exception as e:\n",
    "                errors.append((rel_path, str(e)))\n",
    "        \n",
    "        if not batch_tensors:\n",
    "            continue\n",
    "        \n",
    "        # Stack into batch tensor\n",
    "        batch_tensor = torch.stack(batch_tensors).to(device)\n",
    "        \n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            features = extractor(batch_tensor).cpu().numpy()\n",
    "        \n",
    "        # Scale features\n",
    "        features_scaled = scaler.transform(features)\n",
    "        \n",
    "        all_features.append(features_scaled)\n",
    "        all_paths.extend(batch_paths)\n",
    "        all_labels.extend(batch_labels)\n",
    "        \n",
    "        # Progress\n",
    "        done = min(i + batch_size, total)\n",
    "        elapsed = time.time() - t0\n",
    "        print(f'\\r  Encoded {done}/{total} images ({elapsed:.1f}s)', end='', flush=True)\n",
    "    \n",
    "    print(f'\\nDone! {len(all_paths)} images encoded in {time.time()-t0:.1f}s')\n",
    "    if errors:\n",
    "        print(f'  ⚠️ {len(errors)} errors: {errors[:3]}')\n",
    "    \n",
    "    features_matrix = np.vstack(all_features)\n",
    "    return features_matrix, np.array(all_paths), np.array(all_labels)\n",
    "\n",
    "features, paths, labels = encode_images(\n",
    "    all_images, extractor, scaler, test_transform, DEVICE\n",
    ")\n",
    "print(f'\\nFeatures shape: {features.shape}')\n",
    "print(f'Paths: {paths.shape}, Labels: {labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117961d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to .npz\n",
    "np.savez(\n",
    "    OUTPUT_NPZ,\n",
    "    features=features,\n",
    "    paths=paths,\n",
    "    labels=labels,\n",
    ")\n",
    "file_size_mb = OUTPUT_NPZ.stat().st_size / (1024 * 1024)\n",
    "print(f'Saved: {OUTPUT_NPZ} ({file_size_mb:.1f} MB)')\n",
    "print(f'  features: {features.shape} ({features.dtype})')\n",
    "print(f'  paths: {paths.shape}')\n",
    "print(f'  labels: {labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff2f6f9",
   "metadata": {},
   "source": [
    "## 3. Benchmark des métriques de distance\n",
    "\n",
    "On compare plusieurs métriques pour trouver la plus pertinente pour la recherche de similarité visuelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87d22c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist, cosine, euclidean, cityblock, minkowski\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Reload if needed\n",
    "data = np.load(OUTPUT_NPZ, allow_pickle=True)\n",
    "features = data['features']\n",
    "paths = data['paths']\n",
    "labels = data['labels']\n",
    "print(f'Loaded {features.shape[0]} feature vectors of dim {features.shape[1]}')\n",
    "\n",
    "# Define distance functions\n",
    "METRICS = {\n",
    "    'euclidean': lambda q, db: cdist(q.reshape(1, -1), db, metric='euclidean').flatten(),\n",
    "    'cosine': lambda q, db: cdist(q.reshape(1, -1), db, metric='cosine').flatten(),\n",
    "    'manhattan': lambda q, db: cdist(q.reshape(1, -1), db, metric='cityblock').flatten(),\n",
    "    'minkowski_p3': lambda q, db: cdist(q.reshape(1, -1), db, metric='minkowski', p=3).flatten(),\n",
    "    'correlation': lambda q, db: cdist(q.reshape(1, -1), db, metric='correlation').flatten(),\n",
    "    'chebyshev': lambda q, db: cdist(q.reshape(1, -1), db, metric='chebyshev').flatten(),\n",
    "}\n",
    "\n",
    "def get_top_k(query_idx, metric_fn, features, k=10):\n",
    "    \"\"\"Get top-k most similar images for a query (excluding itself).\"\"\"\n",
    "    query = features[query_idx]\n",
    "    distances = metric_fn(query, features)\n",
    "    # Exclude the query itself\n",
    "    distances[query_idx] = np.inf\n",
    "    top_indices = np.argsort(distances)[:k]\n",
    "    top_distances = distances[top_indices]\n",
    "    return top_indices, top_distances\n",
    "\n",
    "print('Metrics defined:', list(METRICS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4f4e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual benchmark: pick random query images and show top-10 for each metric\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(len(features), size=3, replace=False)\n",
    "\n",
    "for q_idx in sample_indices:\n",
    "    q_path = paths[q_idx]\n",
    "    q_label = labels[q_idx]\n",
    "    print(f'\\n{\"=\"*80}')\n",
    "    print(f'Query: {q_path} (label={q_label})')\n",
    "    print(f'{\"=\"*80}')\n",
    "    \n",
    "    for metric_name, metric_fn in METRICS.items():\n",
    "        top_idx, top_dist = get_top_k(q_idx, metric_fn, features, k=10)\n",
    "        \n",
    "        # Show results as grid\n",
    "        fig, axes = plt.subplots(1, 11, figsize=(22, 2.5))\n",
    "        fig.suptitle(f'{metric_name} — query: {q_path}', fontsize=10, y=1.02)\n",
    "        \n",
    "        # Query image\n",
    "        q_img = Image.open(CASTING_DATA_DIR / q_path)\n",
    "        axes[0].imshow(q_img)\n",
    "        axes[0].set_title(f'QUERY\\n{q_label}', fontsize=8, fontweight='bold', color='blue')\n",
    "        axes[0].axis('off')\n",
    "        axes[0].spines[:].set_visible(False)\n",
    "        \n",
    "        # Top 10\n",
    "        for i, (idx, dist) in enumerate(zip(top_idx, top_dist)):\n",
    "            img = Image.open(CASTING_DATA_DIR / paths[idx])\n",
    "            axes[i+1].imshow(img)\n",
    "            match_color = 'green' if labels[idx] == q_label else 'red'\n",
    "            axes[i+1].set_title(f'#{i+1} d={dist:.3f}\\n{labels[idx]}', fontsize=7, color=match_color)\n",
    "            axes[i+1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40816fe3",
   "metadata": {},
   "source": [
    "## 4. Évaluation quantitative\n",
    "\n",
    "Pour chaque métrique, on mesure :\n",
    "- **Same-class ratio** : % d'images du top-10 qui sont de la même classe que la requête\n",
    "- **Mean distance** : distance moyenne du top-10\n",
    "- **Temps de calcul** par requête\n",
    "\n",
    "On évalue sur un échantillon de N requêtes aléatoires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcaffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative evaluation\n",
    "np.random.seed(123)\n",
    "N_EVAL = min(100, len(features))  # Number of query images for evaluation\n",
    "eval_indices = np.random.choice(len(features), size=N_EVAL, replace=False)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for metric_name, metric_fn in METRICS.items():\n",
    "    same_class_ratios = []\n",
    "    mean_distances = []\n",
    "    t0 = time.time()\n",
    "    \n",
    "    for q_idx in eval_indices:\n",
    "        top_idx, top_dist = get_top_k(q_idx, metric_fn, features, k=10)\n",
    "        # Same-class ratio\n",
    "        q_label = labels[q_idx]\n",
    "        same_class = np.sum(labels[top_idx] == q_label)\n",
    "        same_class_ratios.append(same_class / 10.0)\n",
    "        mean_distances.append(np.mean(top_dist))\n",
    "    \n",
    "    elapsed = (time.time() - t0) / N_EVAL * 1000  # ms per query\n",
    "    \n",
    "    results[metric_name] = {\n",
    "        'same_class_ratio': np.mean(same_class_ratios),\n",
    "        'same_class_std': np.std(same_class_ratios),\n",
    "        'mean_distance': np.mean(mean_distances),\n",
    "        'ms_per_query': elapsed,\n",
    "    }\n",
    "    \n",
    "    print(f'{metric_name:20s} | same-class: {results[metric_name][\"same_class_ratio\"]*100:.1f}% '\n",
    "          f'(±{results[metric_name][\"same_class_std\"]*100:.1f}%) | '\n",
    "          f'mean dist: {results[metric_name][\"mean_distance\"]:.4f} | '\n",
    "          f'{elapsed:.2f} ms/query')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084afa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary chart\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "metric_names = list(results.keys())\n",
    "same_class_vals = [results[m]['same_class_ratio'] * 100 for m in metric_names]\n",
    "time_vals = [results[m]['ms_per_query'] for m in metric_names]\n",
    "\n",
    "colors = ['#3b82f6', '#22c55e', '#eab308', '#ef4444', '#8b5cf6', '#f97316']\n",
    "\n",
    "ax1.barh(metric_names, same_class_vals, color=colors[:len(metric_names)])\n",
    "ax1.set_xlabel('Same-class ratio (%)')\n",
    "ax1.set_title('Cohérence des classes (top-10)')\n",
    "ax1.set_xlim(0, 100)\n",
    "for i, v in enumerate(same_class_vals):\n",
    "    ax1.text(v + 1, i, f'{v:.1f}%', va='center', fontsize=9)\n",
    "\n",
    "ax2.barh(metric_names, time_vals, color=colors[:len(metric_names)])\n",
    "ax2.set_xlabel('Temps (ms/requête)')\n",
    "ax2.set_title('Vitesse de recherche')\n",
    "for i, v in enumerate(time_vals):\n",
    "    ax2.text(v + 0.01, i, f'{v:.2f}ms', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032e5771",
   "metadata": {},
   "source": [
    "## 5. Combinaisons de distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e738c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test weighted combinations of distance metrics\n",
    "def normalize_distances(distances):\n",
    "    \"\"\"Min-max normalize distances to [0, 1].\"\"\"\n",
    "    dmin, dmax = distances.min(), distances.max()\n",
    "    if dmax - dmin < 1e-10:\n",
    "        return np.zeros_like(distances)\n",
    "    return (distances - dmin) / (dmax - dmin)\n",
    "\n",
    "COMBOS = {\n",
    "    'cosine+euclidean_50_50': {'cosine': 0.5, 'euclidean': 0.5},\n",
    "    'cosine+euclidean_70_30': {'cosine': 0.7, 'euclidean': 0.3},\n",
    "    'cosine+manhattan_60_40': {'cosine': 0.6, 'manhattan': 0.4},\n",
    "    'cosine+correlation_50_50': {'cosine': 0.5, 'correlation': 0.5},\n",
    "}\n",
    "\n",
    "def combo_distance(query, db, weights_dict):\n",
    "    \"\"\"Compute weighted combination of normalized distances.\"\"\"\n",
    "    combined = np.zeros(db.shape[0])\n",
    "    for metric_name, weight in weights_dict.items():\n",
    "        raw = METRICS[metric_name](query, db)\n",
    "        combined += weight * normalize_distances(raw)\n",
    "    return combined\n",
    "\n",
    "# Evaluate combinations\n",
    "print('\\n--- Combined Metrics ---')\n",
    "combo_results = {}\n",
    "\n",
    "for combo_name, weights in COMBOS.items():\n",
    "    same_class_ratios = []\n",
    "    t0 = time.time()\n",
    "    \n",
    "    for q_idx in eval_indices:\n",
    "        query = features[q_idx]\n",
    "        distances = combo_distance(query, features, weights)\n",
    "        distances[q_idx] = np.inf\n",
    "        top_idx = np.argsort(distances)[:10]\n",
    "        same_class = np.sum(labels[top_idx] == labels[q_idx])\n",
    "        same_class_ratios.append(same_class / 10.0)\n",
    "    \n",
    "    elapsed = (time.time() - t0) / N_EVAL * 1000\n",
    "    combo_results[combo_name] = {\n",
    "        'same_class_ratio': np.mean(same_class_ratios),\n",
    "        'same_class_std': np.std(same_class_ratios),\n",
    "        'ms_per_query': elapsed,\n",
    "        'weights': weights,\n",
    "    }\n",
    "    \n",
    "    print(f'{combo_name:35s} | same-class: {np.mean(same_class_ratios)*100:.1f}% | {elapsed:.2f} ms/query')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d078cde6",
   "metadata": {},
   "source": [
    "## 6. Sauvegarde de la configuration finale\n",
    "\n",
    "Après analyse des résultats ci-dessus, modifiez la cellule ci-dessous pour choisir la métrique optimale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669cc571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# CHOOSE THE BEST METRIC HERE based on results above\n",
    "# =====================================================\n",
    "\n",
    "# Find best single metric\n",
    "best_single = max(results.items(), key=lambda x: x[1]['same_class_ratio'])\n",
    "print(f'Best single metric: {best_single[0]} ({best_single[1][\"same_class_ratio\"]*100:.1f}%)')\n",
    "\n",
    "# Find best combo\n",
    "if combo_results:\n",
    "    best_combo = max(combo_results.items(), key=lambda x: x[1]['same_class_ratio'])\n",
    "    print(f'Best combination: {best_combo[0]} ({best_combo[1][\"same_class_ratio\"]*100:.1f}%)')\n",
    "else:\n",
    "    best_combo = None\n",
    "\n",
    "# Compare and choose\n",
    "if best_combo and best_combo[1]['same_class_ratio'] > best_single[1]['same_class_ratio'] + 0.01:\n",
    "    # Use combined metric\n",
    "    config = {\n",
    "        'type': 'combined',\n",
    "        'name': best_combo[0],\n",
    "        'weights': best_combo[1]['weights'],\n",
    "        'same_class_ratio': round(best_combo[1]['same_class_ratio'], 4),\n",
    "        'ms_per_query': round(best_combo[1]['ms_per_query'], 3),\n",
    "    }\n",
    "else:\n",
    "    # Use single metric\n",
    "    config = {\n",
    "        'type': 'single',\n",
    "        'name': best_single[0],\n",
    "        'metric': best_single[0],\n",
    "        'same_class_ratio': round(best_single[1]['same_class_ratio'], 4),\n",
    "        'ms_per_query': round(best_single[1]['ms_per_query'], 3),\n",
    "    }\n",
    "\n",
    "print(f'\\nChosen config:')\n",
    "print(json.dumps(config, indent=2))\n",
    "\n",
    "# Save\n",
    "with open(OUTPUT_CONFIG, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f'\\n✅ Config saved to {OUTPUT_CONFIG}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
